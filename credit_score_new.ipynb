{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Score Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "A credit score is a numerical expression based on a level analysis of a person's credit files, to represent the creditworthiness of an individual. A credit score is primarily based on a credit report information typically sourced from credit bureaus.\n",
    "\n",
    "Lenders, such as banks and credit card companies, use credit scores to evaluate the potential risk posed by lending money to consumers and to mitigate losses due to bad debt. Lenders use credit scores to determine who qualifies for a loan, at what interest rate, and what credit limits. Lenders also use credit scores to determine which customers are likely to bring in the most revenue. The use of credit or identity scoring prior to authorizing access or granting credit is an implementation of a trusted system.\n",
    "\n",
    "Credit scoring is not limited to banks. Other organizations, such as mobile phone companies, insurance companies, landlords, and government departments employ the same techniques. Digital finance companies such as online lenders also use alternative data sources to calculate the creditworthiness of borrowers. Credit scoring also has much overlap with data mining, which uses many similar techniques. These techniques combine thousands of factors but are similar or identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Score prediction using Machine Learning\n",
    "We are using some standard machine learning approaches and algorithms in order to predict the credit score of a customer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load all the libraries and other custom written functionalities to our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from collections import Counter, OrderedDict\n",
    "import copy\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import ADASYN\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "from iv import WOE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have all the given datasets on board!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_raw_df = pd.read_csv('references/test_data/raw_data_70_new.csv', low_memory=False)\n",
    "train_account_df = pd.read_csv('references/test_data/raw_account_70_new.csv', low_memory=False)\n",
    "train_enquiry_df = pd.read_csv('references/test_data/raw_enquiry_70_new.csv', low_memory=False)\n",
    "test_raw_df = pd.read_csv('references/test_data/raw_data_30_new.csv', low_memory=False)\n",
    "test_account_df = pd.read_csv('references/test_data/raw_account_30_new.csv', low_memory=False)\n",
    "test_enquiry_df = pd.read_csv('references/test_data/raw_enquiry_30_new.csv', low_memory=False)\n",
    "\n",
    "# Datasets custom prepared\n",
    "dss_train_df = pd.read_csv('references/test_data/dss_data_70_new.csv', low_memory=False)\n",
    "dss_test_df = pd.read_csv('references/test_data/dss_data_30_new.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a custom function created in order to understand the dataset faster and better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def describe(df, shape=True, columns=False, missing_vals=False):\n",
    "    # dimensions of the dataset\n",
    "    print('Data dimensions: ', df.shape, '\\n') if shape else None\n",
    "\n",
    "    # column names\n",
    "    print('Column names: ', df.columns, '\\n') if columns else None\n",
    "\n",
    "    # Number of missing values in each column\n",
    "    print('Number of missing values in each column \\n', df.isnull().sum(axis=0)) if missing_vals else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below function is used to check the dataset's NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_nans = lambda df: df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below function returns the attribute columns in a dataframe by removing the customer ID and the target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_attr_cols(cols):\n",
    "    \"\"\"\n",
    "    Return a list of attribute columns\n",
    "    \"\"\"\n",
    "    non_attr_cols = ['customer_no','Bad_label']\n",
    "    return [col for col in cols if col not in non_attr_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a validity function which can validate the value for None and NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validity(field):\n",
    "    \"\"\"\n",
    "    Returns True for a valid field else False\n",
    "    \"\"\"\n",
    "    validity = True if field is not None and field == field else False\n",
    "    return validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Description\n",
    "\n",
    "Now, let's use the functionality we created to understand the datasets we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Describe the Raw data\n",
    "describe(train_raw_df, columns=True)\n",
    "\n",
    "# Event rate\n",
    "print('Event rate: ',sum(train_raw_df.loc[:, 'Bad_label'])/train_raw_df.shape[0], '\\n')\n",
    "\n",
    "# Describe the accounts data\n",
    "describe(train_account_df)\n",
    "\n",
    "# Describe the enquiry data\n",
    "describe(train_enquiry_df)\n",
    "\n",
    "# Describe the dataset loaded by selecting the best features from raw_df\n",
    "describe(dss_train_df, columns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a function to plot the event rate in the dataframe!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_event_rate(df):\n",
    "    \"\"\"\n",
    "    Plots the event rate in a bar plot\n",
    "    \"\"\"\n",
    "    pd.value_counts(df['Bad_label']).plot.bar()\n",
    "    plt.title('Distribution of customer Bad_label in the balanced dataset')\n",
    "    plt.xlabel('Bad_label')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_event_rate(train_raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Processing\n",
    "\n",
    "Now is the time we have to analyze the datasets and understand.\n",
    "\n",
    "But lets do things bit hacky as there is shortage of time:P As we already got some clues of analysis we know the important variables which can avoid a lot of frustrations and disappointments during our course of building the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2.1 Feature Generation\n",
    "Lets first create a set of functionalities required for our processing of the derived features. All the methods below have a comment string stating their purpose.\n",
    "\n",
    "Here the intention is not performance/efficiency but the readability. Hence there are many places where we can see some repeated operations which are trivial to refactor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The total duration between last payment date and account opened date of all accounts\n",
    "def diff_last_pmt_dt_opnd_dt(last_pmt_dt, opened_dt):\n",
    "    \"\"\"\n",
    "    The difference between last payment date and account opened date in days\n",
    "    \"\"\"\n",
    "    num_days = 0\n",
    "    if opened_dt is not None and opened_dt == opened_dt:\n",
    "        opened_date = datetime.datetime.strptime(opened_dt, \"%d-%b-%y\")\n",
    "        if last_pmt_dt is not None and last_pmt_dt == last_pmt_dt:\n",
    "            last_payment_date = datetime.datetime.strptime(last_pmt_dt, \"%d-%b-%y\")\n",
    "            diff = last_payment_date - opened_date\n",
    "            num_days = diff.days\n",
    "    return num_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The mean count of accounts that is in 0-29 dpd bucket throughout the payment history\n",
    "def count_30_dpd(paymenthistory1, paymenthistory2):\n",
    "    \"\"\"\n",
    "    The mean count of accounts that is in 0-29 dpd bucket throughout the payment history\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    paymenthistory1 = paymenthistory1.strip(\"\\\"\") if paymenthistory1 == paymenthistory1 else \"\"\n",
    "    paymenthistory2 = paymenthistory2.strip(\"\\\"\") if paymenthistory2 == paymenthistory2 else \"\"\n",
    "    paymenthistory = paymenthistory1 + paymenthistory2\n",
    "    for i in range(0, len(paymenthistory), 3):\n",
    "        x = paymenthistory[i:i+3]\n",
    "        if x.isdigit():\n",
    "            if int(x) < 30:\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The smallest number of months passed before first 30+ dpd appeared for each account.\n",
    "def min_months_30_dpd(paymenthistory1, paymenthistory2):\n",
    "    \"\"\"\n",
    "    The minimum number of months happened before first 30plus dpd has happened\n",
    "    \"\"\"\n",
    "    months = 0\n",
    "    paymenthistory1 = paymenthistory1.strip(\"\\\"\") if paymenthistory1 == paymenthistory1 else \"\"\n",
    "    paymenthistory2 = paymenthistory2.strip(\"\\\"\") if paymenthistory2 == paymenthistory2 else \"\"\n",
    "    paymenthistory = paymenthistory1 + paymenthistory2\n",
    "    for i in range(0, len(paymenthistory), 3):\n",
    "        x = paymenthistory[i:i+3]\n",
    "        if x.isdigit():\n",
    "            if int(x) < 30:\n",
    "                months += 1\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            months += 1\n",
    "    return months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of enquiries made in past 365 days\n",
    "def count_past_365_days(opened_dt_series, enquiry_dt_series):\n",
    "    \"\"\"\n",
    "    Return 1 if the date difference is under 365 days else 0\n",
    "    \"\"\"\n",
    "    num_of_enq = 0\n",
    "    for opened_dt, enquiry_dt in zip(opened_dt_series, enquiry_dt_series):\n",
    "        if opened_dt is not None and opened_dt == opened_dt and enquiry_dt is not None and enquiry_dt == enquiry_dt:\n",
    "            opened_date = datetime.datetime.strptime(opened_dt, \"%d-%b-%y\")\n",
    "            enquiry_date = datetime.datetime.strptime(enquiry_dt, \"%d-%b-%y\")\n",
    "            diff = enquiry_date - opened_date\n",
    "            if diff.days <= 365:\n",
    "                num_of_enq += 1\n",
    "    return num_of_enq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ratio of total current balance amount to total credit limit\n",
    "def ratio_cur_bal_credit(cur_bal_series, credit_limit_series):\n",
    "    \"\"\"\n",
    "    Calculating total current_balance_amout / total credit_limit\n",
    "    \"\"\"\n",
    "    ratio = cur_bal_series.sum() / credit_limit_series.sum()\n",
    "    if ratio != ratio:\n",
    "        return 0\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The mean duration between last payment date and account opened date of all accounts\n",
    "def mean_last_pmt_acc_opnd(last_pmt_dt_series, acc_opnd_dt_series):\n",
    "    \"\"\"\n",
    "    Calculating the mean duration between last payment and account opened date for all accounts of a customer\n",
    "    \"\"\"\n",
    "    mean_vals = []\n",
    "    for last_pmt_dt, acc_opnd_dt in zip(last_pmt_dt_series, acc_opnd_dt_series):\n",
    "        if last_pmt_dt is not None and last_pmt_dt == last_pmt_dt and acc_opnd_dt is not None and acc_opnd_dt == acc_opnd_dt:\n",
    "            last_pmt_date = datetime.datetime.strptime(last_pmt_dt, \"%d-%b-%y\")\n",
    "            acc_opnd_date = datetime.datetime.strptime(acc_opnd_dt, \"%d-%b-%y\")\n",
    "            difference = last_pmt_date - acc_opnd_date\n",
    "            mean_vals.append(difference.days)\n",
    "    return int(sum(mean_vals) / len(mean_vals)) if len(mean_vals) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The average difference between enquiry dt_opened date and enquiry date\n",
    "def diff_enq_dt_opened_enq_dt(dt_opened, enq_date):\n",
    "    \"\"\"\n",
    "    The days difference between date_opened in enquiry and the actual enquiry_date\n",
    "    \"\"\"\n",
    "    num_days = 0\n",
    "    if dt_opened is not None and dt_opened == dt_opened:\n",
    "        opened_date = datetime.datetime.strptime(dt_opened, \"%d-%b-%y\")\n",
    "        if enq_date is not None and enq_date == enq_date:\n",
    "            enquiry_date = datetime.datetime.strptime(enq_date, \"%d-%b-%y\")\n",
    "            diff = opened_date - enquiry_date\n",
    "            num_days = diff.days\n",
    "    return num_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Average length of payment history variable\n",
    "def avg_length_payment_history(paymenthistory1, paymenthistory2):\n",
    "    \"\"\"\n",
    "    Average length of payment history variable\n",
    "    \"\"\"\n",
    "    length = 0\n",
    "    paymenthistory1 = paymenthistory1.strip(\"\\\"\") if paymenthistory1 == paymenthistory1 else \"\"\n",
    "    paymenthistory2 = paymenthistory2.strip(\"\\\"\") if paymenthistory2 == paymenthistory2 else \"\"\n",
    "    paymenthistory = paymenthistory1 + paymenthistory2\n",
    "    for i in range(0, len(paymenthistory), 3):\n",
    "        length += 1\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Most frequent enquiry purpose\n",
    "def mode_enq_purpose(enq_purpose):\n",
    "    \"\"\"\n",
    "    Return the mode of the enquiry purpose for a customer\n",
    "    \"\"\"\n",
    "    count = Counter(enq_purpose)\n",
    "    return max(count, key=count.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of enquiry made in past 90 days\n",
    "def count_past_90_days(opened_dt_series, enquiry_dt_series):\n",
    "    \"\"\"\n",
    "    Return 1 if the date difference is under 90 days else 0\n",
    "    \"\"\"\n",
    "    num_of_enq = 0\n",
    "    for opened_dt, enquiry_dt in zip(opened_dt_series, enquiry_dt_series):\n",
    "        if opened_dt is not None and opened_dt == opened_dt and enquiry_dt is not None and enquiry_dt == enquiry_dt:\n",
    "            opened_date = datetime.datetime.strptime(opened_dt, \"%d-%b-%y\")\n",
    "            enquiry_date = datetime.datetime.strptime(enquiry_dt, \"%d-%b-%y\")\n",
    "            if (int(enquiry_date.timestamp()) - int(opened_date.timestamp())) <= 90:\n",
    "                num_of_enq += 1\n",
    "    return num_of_enq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the utilization trend\n",
    "def utilization_trend(cur_balance_amt_series, credit_limit_series, cash_limit_series):\n",
    "    \"\"\"\n",
    "    Calculat the utilization trend of a customer\n",
    "    \"\"\"\n",
    "    total_cur_balance_amt = cur_balance_amt_series.sum()\n",
    "    total_credit_limit = credit_limit_series.sum()\n",
    "    mean_cur_balance_amt = cur_balance_amt_series.mean()\n",
    "    mean_credit_limit = credit_limit_series.mean()\n",
    "    mean_cash_limit = cash_limit_series.mean()\n",
    "    return (total_cur_balance_amt / total_credit_limit) / (mean_cur_balance_amt / (mean_credit_limit + mean_cash_limit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a functionality which can manage all the feature generation and creates and final dataset needed.\n",
    "\n",
    "Just to make our life easier on the test dataset:P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(raw_df: pd.DataFrame, account_df: pd.DataFrame, enquiry_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    This is the main method for the pre-processing of the data.\n",
    "    This method returns a single Pandas DataFrame after the pre processing\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add only the customer_no and Bad_label to the new_df\n",
    "    new_df = pd.DataFrame(data=raw_df[['customer_no','Bad_label']], columns=['customer_no','Bad_label'])\n",
    "    \n",
    "    account_df['diff_last_pmt_dt_opnd_dt'] = account_df.apply(lambda x: diff_last_pmt_dt_opnd_dt(x['last_paymt_dt'], x['opened_dt']), axis=1)\n",
    "    total_diff_last_pmt_dt_opnd_dt_df = account_df[['diff_last_pmt_dt_opnd_dt','customer_no']].groupby('customer_no').sum().reset_index()\n",
    "    total_diff_last_pmt_dt_opnd_dt_df.columns = ['customer_no','diff_last_pmt_dt_opnd_dt']\n",
    "    new_df = new_df.merge(right=total_diff_last_pmt_dt_opnd_dt_df, on='customer_no',how='inner')\n",
    "    \n",
    "    account_df['months_before_first_30dpd'] = account_df.apply(lambda x: count_30_dpd(x['paymenthistory1'], x['paymenthistory2']), axis=1)\n",
    "    min_months_30_dpd_df = account_df[['months_before_first_30dpd','customer_no']].groupby('customer_no').min().reset_index()\n",
    "    min_months_30_dpd_df.columns = ['customer_no','min_months_before_first_30dpd']\n",
    "    new_df = new_df.merge(right=min_months_30_dpd_df, on='customer_no',how='inner')\n",
    "    \n",
    "    enquiry_recency_365_df = enquiry_df.groupby('customer_no').apply(lambda x: count_past_365_days(x['dt_opened'],x['enquiry_dt'])).to_frame().reset_index()\n",
    "    enquiry_recency_365_df.columns = ['customer_no','enquiry_recency_365']\n",
    "    new_df = new_df.merge(right=enquiry_recency_365_df, on='customer_no',how='inner')\n",
    "    \n",
    "    ratio_cur_bal_credit_df = account_df.groupby('customer_no').apply(lambda x: ratio_cur_bal_credit(x['cur_balance_amt'], x['creditlimit'])).to_frame().reset_index()\n",
    "    ratio_cur_bal_credit_df.columns = ['customer_no','ratio_cur_bal_credit']\n",
    "    new_df = new_df.merge(right=ratio_cur_bal_credit_df, on='customer_no',how='inner')\n",
    "    \n",
    "    mean_last_pmt_acc_opnd_df = account_df.groupby('customer_no').apply(lambda x: mean_last_pmt_acc_opnd(x['last_paymt_dt'], x['opened_dt'])).to_frame().reset_index()\n",
    "    mean_last_pmt_acc_opnd_df.columns = ['customer_no','mean_last_pmt_acc_opnd']\n",
    "    new_df = new_df.merge(right=mean_last_pmt_acc_opnd_df, on='customer_no',how='inner')\n",
    "\n",
    "    enquiry_df['diff_opened_dt_enq_dt'] = enquiry_df.apply(lambda x: diff_enq_dt_opened_enq_dt(x['dt_opened'], x['enquiry_dt']), axis=1)\n",
    "    avg_diff_enq_dt_opened_enq_dt_df = enquiry_df[['customer_no','diff_opened_dt_enq_dt']].groupby('customer_no').mean().reset_index()\n",
    "    avg_diff_enq_dt_opened_enq_dt_df.columns = ['customer_no','mean_diff_opened_dt_enq_dt']\n",
    "    new_df = new_df.merge(right=avg_diff_enq_dt_opened_enq_dt_df, on='customer_no',how='inner')\n",
    "\n",
    "    account_df['pmt_history_len'] = account_df.apply(lambda x: avg_length_payment_history(x['paymenthistory1'], x['paymenthistory2']), axis=1)\n",
    "    pmt_history_len_df = account_df[['customer_no','pmt_history_len']].groupby('customer_no').mean().reset_index()\n",
    "    pmt_history_len_df.columns = ['customer_no','mean_pmt_history_len']\n",
    "    new_df = new_df.merge(right=pmt_history_len_df, on='customer_no',how='inner')\n",
    "    \n",
    "    mode_enq_purpose_df = enquiry_df.groupby('customer_no').apply(lambda x: mode_enq_purpose(x['enq_purpose'])).to_frame().reset_index()\n",
    "    mode_enq_purpose_df.columns = ['customer_no','mode_enq_purpose']\n",
    "    new_df = new_df.merge(right=mode_enq_purpose_df, on='customer_no',how='inner')\n",
    "    \n",
    "    enquiry_recency_90_df = enquiry_df.groupby('customer_no').apply(lambda x: count_past_365_days(x['dt_opened'],x['enquiry_dt'])).to_frame().reset_index()\n",
    "    enquiry_recency_90_df.columns = ['customer_no','enquiry_recency_90']\n",
    "    new_df = new_df.merge(right=enquiry_recency_90_df, on='customer_no',how='inner')\n",
    "    \n",
    "    utilization_trend_df = account_df.groupby('customer_no').apply(lambda x: utilization_trend(x['cur_balance_amt'],x['creditlimit'],x['cashlimit'])).to_frame().reset_index()\n",
    "    utilization_trend_df.columns = ['customer_no','utilization_trend']\n",
    "    new_df = new_df.merge(right=utilization_trend_df, on='customer_no', how='inner')\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the processing step and get the new features!! Please remain calm as I already stated that this is not performace specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df_1 = preprocess(raw_df=train_raw_df, account_df=train_account_df, enquiry_df=train_enquiry_df)\n",
    "train_df_1.to_csv('train_df_1.csv', index=False)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the generated features in the dataset.\n",
    "\n",
    "Let's create a single train dataset by combining the features generated dataset and the analyzed and selected dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Concatenate both train_dfs to one train_df\n",
    "# Also removing Bad_label as we already have in the train_df_1\n",
    "train_df = train_df_1.merge(dss_train_df.drop(['Bad_label'], axis=1), on='customer_no', how='inner')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save data as csv\n",
    "train_df.to_csv('train_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a functionality to encode the categorical columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_columns(df, cols_to_encode):\n",
    "    \"\"\"\n",
    "    Encodes all the columns and returns the encoded df\n",
    "    \"\"\"\n",
    "    all_columns = df.columns.tolist()\n",
    "    for col in cols_to_encode:\n",
    "        if col not in all_columns:\n",
    "            print(\"Given column {0} to encode is not in the dataframe's columns {1}\".format(col, all_columns))\n",
    "            continue\n",
    "        df = pd.concat([df, pd.get_dummies(df.loc[:,col], prefix=col)], axis=1)\n",
    "        df.drop([col], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = encode_columns(df=train_df, cols_to_encode=['feature_1','feature_27','feature_32','feature_58','feature_60'])\n",
    "train_df.to_csv('train_df_encoded.csv', index=False)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 NaN and Outlier treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "describe(train_df, missing_vals=True)\n",
    "get_nans(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Impute the mode value of the column to the missing values. Here the code 10.0 is of the high frequency of 19623. \n",
    "# So imputing this value will not make much of a difference to the data\n",
    "\n",
    "train_df['mode_enq_purpose'].fillna(train_df.mode(axis=0).iloc[0].loc['mode_enq_purpose'], inplace=True)\n",
    "\n",
    "# Impute mean value for utilization_trend column\n",
    "train_df['utilization_trend'].fillna(train_df.loc[:,'utilization_trend'].mean(), inplace=True)\n",
    "\n",
    "# Verifying the missing values\n",
    "describe(train_df, columns=True, missing_vals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop records with outliers \n",
    "train_df_otlr_treat = train_df[(np.abs(stats.zscore(train_df)) < 3).all(axis=1)]\n",
    "\n",
    "# event rate\n",
    "print('Event rate: ',sum(train_df_otlr_treat.loc[:, 'Bad_label'])/train_df_otlr_treat.shape[0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will neglect the outlier removal as removing outliers is resulting in the 0.0 event rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a functionality to normalize the dataset's attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "def normalize(df, cols):\n",
    "    \"\"\"\n",
    "    This function normalizes the given cols of the data in the dataset.\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        if col not in df.columns.tolist():\n",
    "            raise Exception(\"The column {0} is not in the DataFrame to normalize\".format(col))\n",
    "        df['norm_' + col] = StandardScaler().fit_transform(df[col].values.reshape(-1, 1))\n",
    "        df.drop([col], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols_to_norm = ['diff_last_pmt_dt_opnd_dt',\n",
    "       'min_months_before_first_30dpd', 'enquiry_recency_365',\n",
    "       'ratio_cur_bal_credit', 'mean_last_pmt_acc_opnd',\n",
    "       'mean_diff_opened_dt_enq_dt', 'mean_pmt_history_len',\n",
    "       'mode_enq_purpose', 'enquiry_recency_90', 'utilization_trend',\n",
    "       'feature_3', 'feature_7', 'feature_29', 'feature_34', 'feature_44',\n",
    "       'feature_64', 'feature_65']\n",
    "normalize(train_df, cols=cols_to_norm)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Calculate Information Value score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a functionality to get the features in the order of their IV scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features_ordered(scores, features):\n",
    "    \"\"\"\n",
    "    This method returns the top n features based on the IV scores.\n",
    "    Returns the list of features in the order of their IV scores. [a, ..., z] a is highest IV and z is lowest IV.\n",
    "    \"\"\"\n",
    "    if not len(features) == len(scores):\n",
    "        print(\"Given number of scores doesn't match with number of features\")\n",
    "        return None\n",
    "    attr_to_score = dict(zip(features, scores))\n",
    "#     return sorted(attr_to_score, key=attr_to_score.get, reverse=True)\n",
    "    features_ordered =  sorted(attr_to_score, key=attr_to_score.get, reverse=True)\n",
    "    features_rank = OrderedDict()\n",
    "    for feature in features_ordered:\n",
    "        features_rank[feature] = attr_to_score[feature]\n",
    "    return features_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iv = WOE()\n",
    "iv_scores = iv.woe(X=train_df.iloc[:,2:].values, y=train_df.iloc[:,1].values)\n",
    "get_top_n_features(scores=iv_scores, features=train_df.columns.tolist()[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Data Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Event rate: ',(sum(train_df.loc[:, 'Bad_label'])/train_df.shape[0]) * 100, '\\n')\n",
    "plot_event_rate(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using ADASYN for the data balancing\n",
    "X_resampled, y_resampled = ADASYN().fit_sample(train_df.iloc[:,2:], train_df.loc[:,'Bad_label'])\n",
    "print('Event rate after data balancing: ', round(100*sum(y_resampled)/y_resampled.shape[0], 1))\n",
    "col_names = train_df.columns\n",
    "train_df_blncd = pd.DataFrame(X_resampled)\n",
    "train_df_blncd.columns = col_names[2:]\n",
    "train_df_blncd.loc[:,'Bad_label'] = y_resampled\n",
    "train_df_blncd.loc[:,'customer_no'] = [i+1 for i in train_df_blncd.index]\n",
    "plot_event_rate(train_df_blncd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the IV scores on the balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iv_scores = iv.woe(X=train_df_blncd.iloc[:,:-2].values, y=train_df_blncd.iloc[:,-2].values)\n",
    "get_top_n_features(scores=iv_scores, features=train_df_blncd.columns.tolist()[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save balanced data as csv\n",
    "train_df_blncd.to_csv('train_df_blncd.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df_blncd = pd.read_csv('train_df_blncd.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Building\n",
    "\n",
    "Let's do some model fitting to the data we prepared till now and see how can we use machine learning for out credit score prediction!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Logistic Regression\n",
    "\n",
    "As we all know one of the trivial solution for the binary classiffication is the Logistic Regression. Let's see how does it perform on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate the logreg model\n",
    "logreg_classiffier = LogisticRegression(penalty='l2', dual=False, tol=0.01, C=10, fit_intercept=True, \n",
    "                        intercept_scaling=1, class_weight=None, random_state=None, \n",
    "                        solver='liblinear', max_iter=100, multi_class='ovr', \n",
    "                        verbose=0, warm_start=False, n_jobs=1)\n",
    "# Fit the model\n",
    "logreg_classiffier.fit(train_df.iloc[:,2:], train_df.iloc[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fit a logreg model for our data now is the time that we actually check how good has been our model fit. \n",
    "\n",
    "Lets cleanup the test data and follow the data processing steps followed for training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pre process the test data\n",
    "test_df_1 = preprocess(raw_df = test_raw_df, account_df = test_account_df, enquiry_df = test_enquiry_df)\n",
    "test_df_2 = dss_test_df.drop(['Bad_label'], axis=1)\n",
    "test_df = test_df_1.merge(test_df_2, on='customer_no', how='inner')\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the test_df as csv\n",
    "test_df.to_csv('test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encode the categorical columns in test data\n",
    "test_df = encode_columns(df=test_df, cols_to_encode=['feature_1','feature_27','feature_32','feature_58','feature_60'])\n",
    "\n",
    "# Impute the mode value of the column to the missing values.\n",
    "test_df['mode_enq_purpose'].fillna(test_df.mode(axis=0).iloc[0].loc['mode_enq_purpose'], inplace=True)\n",
    "#Impute the mean value for utilization_trend\n",
    "test_df['utilization_trend'].fillna(test_df.loc[:,'utilization_trend'].mean(), inplace=True)\n",
    "\n",
    "# Normalize the test data\n",
    "normalize(test_df, cols_to_norm)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_cols = train_df.columns.tolist()\n",
    "te_cols = test_df.columns.tolist()\n",
    "col_not_in_tr = [col for col in te_cols if col not in tr_cols]\n",
    "col_not_in_te = [col for col in tr_cols if col not in te_cols]\n",
    "print(\"Col not in train: {0} and Col not in test: {1}\".format(col_not_in_tr, col_not_in_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that the feature_27_Architect is there in the train data but not in the test data and simillarly the feature_27_Lawyer is vice-versa. \n",
    "\n",
    "As a simple step let's remove the feature_27_Architect from out training data and train the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.drop('feature_27_Architect', axis=1, inplace=True)\n",
    "test_df.drop('feature_27_Lawyer', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets fit our Logreg model again!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.01,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the logreg model\n",
    "logreg_classiffier_1 = LogisticRegression(penalty='l2', dual=False, tol=0.01, C=10, fit_intercept=True, \n",
    "                        intercept_scaling=1, class_weight=None, random_state=None, \n",
    "                        solver='liblinear', max_iter=100, multi_class='ovr', \n",
    "                        verbose=0, warm_start=False, n_jobs=1)\n",
    "# Fit the model\n",
    "logreg_classiffier_1.fit(train_df.iloc[:,2:], train_df.iloc[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the model's fit on the test data and check the quality of our fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictions_lr_1 = logreg_classiffier_1.predict(test_df.iloc[:,2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Good Customer       0.95      1.00      0.98      9778\n",
      " Bad Customer       0.00      0.00      0.00       462\n",
      "\n",
      "  avg / total       0.91      0.95      0.93     10240\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Get classification report\n",
    "target_names=['Good Customer', 'Bad Customer']\n",
    "print(classification_report(test_df.loc[:,'Bad_label'], predictions_lr_1, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a functionality for calculating the AUC score and Gini coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_auc_gini(actual, prediction):\n",
    "    \"\"\"\n",
    "    Prints the AUC and Gini values for the classiffication\n",
    "    \"\"\"\n",
    "    auc_score = roc_auc_score(actual, prediction)\n",
    "    gini = 2 * auc_score - 1\n",
    "    print(\"AUC score: {0}, Gini: {1}\".format(auc_score, gini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_auc_gini(actual=test_df.loc[:,'Bad_label'].tolist(), prediction=list(predictions_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops!! Our firsl model went really bad :( :( :(\n",
    "\n",
    "Thats okay its highly unreasonable to get the best fit in the first model.\n",
    "\n",
    "Let's try to understand the problem better.\n",
    "\n",
    "So first let's see how does our model behave on the balanced data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.01,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the feature_27_Architect from the balanced dataset\n",
    "train_df_blncd.drop('feature_27_Architect', axis=1, inplace=True)\n",
    "train_df_blncd = shuffle(train_df_blncd)\n",
    "\n",
    "# Instantiate the logreg model\n",
    "logreg_classiffier_2 = LogisticRegression(penalty='l2', dual=False, tol=0.01, C=10, fit_intercept=True, \n",
    "                        intercept_scaling=1, class_weight=None, random_state=None, \n",
    "                        solver='liblinear', max_iter=100, multi_class='ovr', \n",
    "                        verbose=0, warm_start=False, n_jobs=1)\n",
    "# Fit the model\n",
    "logreg_classiffier_2.fit(train_df_blncd.iloc[:,:-2], train_df_blncd.iloc[:,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictions_lr_2 = logreg_classiffier_2.predict(test_df.iloc[:,2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Good Customer       0.96      0.74      0.83      9778\n",
      " Bad Customer       0.05      0.28      0.08       462\n",
      "\n",
      "  avg / total       0.91      0.72      0.80     10240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get classification report\n",
    "target_names=['Good Customer', 'Bad Customer']\n",
    "print(classification_report(test_df.loc[:,'Bad_label'], predictions_lr_2, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score: 0.5090110850491296, Gini: 0.018022170098259238\n"
     ]
    }
   ],
   "source": [
    "get_auc_gini(actual=test_df.loc[:,'Bad_label'].tolist(), prediction=list(predictions_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool!! We could find some benefit using the oversampling technique. But thats not quite enough\n",
    "\n",
    "Let's see if some other classiffies can improve the performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a Random Forest classiffier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, \n",
    "                                min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                                max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                                min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=-1, \n",
    "                                random_state=42, verbose=0, warm_start=False, class_weight=None)\n",
    "\n",
    "# Fit the model\n",
    "rf_clf.fit(train_df_blncd.iloc[:,:-2], train_df_blncd.iloc[:,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predictions\n",
    "predictions_rf = rf_clf.predict(test_df.iloc[:,:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Good Customer       0.96      0.52      0.68      9778\n",
      " Bad Customer       0.06      0.59      0.10       462\n",
      "\n",
      "  avg / total       0.92      0.53      0.65     10240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get classification report\n",
    "target_names=['Good Customer', 'Bad Customer']\n",
    "print(classification_report(test_df.loc[:,'Bad_label'], predictions_rf, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score: 0.5584001632784615, Gini: 0.11680032655692307\n"
     ]
    }
   ],
   "source": [
    "get_auc_gini(actual=test_df.loc[:,'Bad_label'].tolist(), prediction=list(predictions_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats pretty much a good news to what we were seeing earlier.\n",
    "\n",
    "So far we have used a logistic model and a tree based model. Logistic regression is a parametric model while random forest is a non-parametric model. Next we will train a boosted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=150, random_state=None)"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the parameters\n",
    "n_estimators = 150\n",
    "learning_rate = 1.\n",
    "\n",
    "# train the adaboost algorithm\n",
    "ada_real = AdaBoostClassifier(\n",
    "    learning_rate=learning_rate,\n",
    "    n_estimators=n_estimators,\n",
    "    algorithm=\"SAMME.R\")\n",
    "\n",
    "ada_real.fit(train_df_blncd.iloc[:,:-2], train_df_blncd.iloc[:,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictions_ab = ada_real.predict(test_df.iloc[:,:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Good Customer       0.97      0.48      0.64      9778\n",
      " Bad Customer       0.06      0.71      0.11       462\n",
      "\n",
      "  avg / total       0.93      0.49      0.62     10240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get classification report\n",
    "target_names=['Good Customer', 'Bad Customer']\n",
    "print(classification_report(test_df.loc[:,'Bad_label'], predictions_ab, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score: 0.5969158168483184, Gini: 0.1938316336966368\n"
     ]
    }
   ],
   "source": [
    "get_auc_gini(actual=test_df.loc[:,'Bad_label'].tolist(), prediction=list(predictions_ab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats pretty cool!!\n",
    "\n",
    "Let's wrap up everything with a last good fit model\n",
    "\n",
    "Now let's try with an ensemble model and see if it helps in anyway to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.01,\n",
       "          verbose=0, warm_start=False)), ('rf', RandomFore...='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=150, random_state=None))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate an ensemble classiffier\n",
    "e_classiffier = VotingClassifier(estimators=[('lr', logreg_classiffier_2), ('rf', rf_clf), ('ab', ada_real)])\n",
    "\n",
    "# Fit the train data\n",
    "e_classiffier.fit(train_df_blncd.iloc[:,:-2], train_df_blncd.iloc[:,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictions_ec = e_classiffier.predict(test_df.iloc[:,:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Good Customer       0.97      0.34      0.50      9778\n",
      " Bad Customer       0.05      0.77      0.10       462\n",
      "\n",
      "  avg / total       0.93      0.36      0.48     10240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get classification report\n",
    "target_names=['Good Customer', 'Bad Customer']\n",
    "print(classification_report(test_df.loc[:,'Bad_label'], predictions_ec, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score: 0.5562859108573979, Gini: 0.11257182171479574\n"
     ]
    }
   ],
   "source": [
    "get_auc_gini(actual=test_df.loc[:,'Bad_label'].tolist(), prediction=list(predictions_ec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":( :( This ensembling didn't help though\n",
    "\n",
    "Anyway the Adaboost is so far giving a real good result. Let's try a small variation on the Adaboost w.r.t data.\n",
    "\n",
    "Instead of taking all the attributes let's only take the top 10 attributes from IV and see if that helps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_32_Self',\n",
       " 'feature_1_Platinum Maxima',\n",
       " 'feature_1_Titanium Deligh',\n",
       " 'feature_27_Graduate',\n",
       " 'feature_1_Platinum Deligh',\n",
       " 'norm_mode_enq_purpose',\n",
       " 'norm_feature_65',\n",
       " 'norm_feature_29',\n",
       " 'norm_feature_64',\n",
       " 'norm_feature_44',\n",
       " 'norm_utilization_trend',\n",
       " 'feature_32_Rente',\n",
       " 'feature_32_Paren',\n",
       " 'norm_mean_pmt_history_len',\n",
       " 'norm_ratio_cur_bal_credit',\n",
       " 'norm_mean_diff_opened_dt_enq_dt',\n",
       " 'norm_diff_last_pmt_dt_opnd_dt',\n",
       " 'norm_feature_3',\n",
       " 'norm_mean_last_pmt_acc_opnd',\n",
       " 'norm_enquiry_recency_365',\n",
       " 'norm_enquiry_recency_90',\n",
       " 'norm_feature_34',\n",
       " 'feature_58_N',\n",
       " 'norm_feature_7',\n",
       " 'norm_min_months_before_first_30dpd']"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iv_scores = iv.woe(X=train_df_blncd.iloc[:,:-2].values, y=train_df_blncd.iloc[:,-2].values)\n",
    "ordered_dict = get_top_n_features(scores=iv_scores, features=train_df_blncd.columns.tolist()[:-2])\n",
    "cols_to_select = []\n",
    "for k, v in ordered_dict.items():\n",
    "    if v > 0.009:\n",
    "        cols_to_select.append(k)\n",
    "cols_to_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets select only these columns from our training set and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_32_Self</th>\n",
       "      <th>feature_1_Platinum Maxima</th>\n",
       "      <th>feature_1_Titanium Deligh</th>\n",
       "      <th>feature_27_Graduate</th>\n",
       "      <th>feature_1_Platinum Deligh</th>\n",
       "      <th>norm_mode_enq_purpose</th>\n",
       "      <th>norm_feature_65</th>\n",
       "      <th>norm_feature_29</th>\n",
       "      <th>norm_feature_64</th>\n",
       "      <th>norm_feature_44</th>\n",
       "      <th>...</th>\n",
       "      <th>norm_feature_3</th>\n",
       "      <th>norm_mean_last_pmt_acc_opnd</th>\n",
       "      <th>norm_enquiry_recency_365</th>\n",
       "      <th>norm_enquiry_recency_90</th>\n",
       "      <th>norm_feature_34</th>\n",
       "      <th>feature_58_N</th>\n",
       "      <th>norm_feature_7</th>\n",
       "      <th>norm_min_months_before_first_30dpd</th>\n",
       "      <th>Bad_label</th>\n",
       "      <th>customer_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28346</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.044563</td>\n",
       "      <td>-0.005440</td>\n",
       "      <td>0.108638</td>\n",
       "      <td>0.014463</td>\n",
       "      <td>0.135921</td>\n",
       "      <td>...</td>\n",
       "      <td>1.212834</td>\n",
       "      <td>2.539525</td>\n",
       "      <td>-0.598901</td>\n",
       "      <td>-0.598901</td>\n",
       "      <td>-0.555239</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.530242</td>\n",
       "      <td>2.184268</td>\n",
       "      <td>1</td>\n",
       "      <td>28347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24850</th>\n",
       "      <td>0.696036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.696036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.761931</td>\n",
       "      <td>-0.248382</td>\n",
       "      <td>-0.652147</td>\n",
       "      <td>-0.385947</td>\n",
       "      <td>-0.692244</td>\n",
       "      <td>...</td>\n",
       "      <td>1.538943</td>\n",
       "      <td>-0.029341</td>\n",
       "      <td>0.922180</td>\n",
       "      <td>0.922180</td>\n",
       "      <td>1.063801</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.106040</td>\n",
       "      <td>-0.363198</td>\n",
       "      <td>1</td>\n",
       "      <td>24851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.380800</td>\n",
       "      <td>-0.746978</td>\n",
       "      <td>1.146315</td>\n",
       "      <td>0.674171</td>\n",
       "      <td>1.133739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082505</td>\n",
       "      <td>-0.202718</td>\n",
       "      <td>5.226114</td>\n",
       "      <td>5.226114</td>\n",
       "      <td>1.770848</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.149356</td>\n",
       "      <td>-0.577087</td>\n",
       "      <td>0</td>\n",
       "      <td>2302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42220</th>\n",
       "      <td>0.692261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.307739</td>\n",
       "      <td>0.307739</td>\n",
       "      <td>0.692261</td>\n",
       "      <td>0.258746</td>\n",
       "      <td>-0.162771</td>\n",
       "      <td>-0.675146</td>\n",
       "      <td>0.326701</td>\n",
       "      <td>-0.662817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.229240</td>\n",
       "      <td>-0.605799</td>\n",
       "      <td>-0.351662</td>\n",
       "      <td>-0.351662</td>\n",
       "      <td>-0.555239</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.055496</td>\n",
       "      <td>-0.522950</td>\n",
       "      <td>1</td>\n",
       "      <td>42221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15621</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.258746</td>\n",
       "      <td>-0.275564</td>\n",
       "      <td>-0.675038</td>\n",
       "      <td>1.935202</td>\n",
       "      <td>-0.639324</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.859407</td>\n",
       "      <td>-0.692453</td>\n",
       "      <td>-0.739956</td>\n",
       "      <td>-0.739956</td>\n",
       "      <td>-0.555239</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.775151</td>\n",
       "      <td>0.478410</td>\n",
       "      <td>0</td>\n",
       "      <td>15622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature_32_Self  feature_1_Platinum Maxima  feature_1_Titanium Deligh  \\\n",
       "28346         0.000000                        0.0                   1.000000   \n",
       "24850         0.696036                        0.0                   1.000000   \n",
       "2301          0.000000                        0.0                   1.000000   \n",
       "42220         0.692261                        0.0                   0.307739   \n",
       "15621         0.000000                        0.0                   0.000000   \n",
       "\n",
       "       feature_27_Graduate  feature_1_Platinum Deligh  norm_mode_enq_purpose  \\\n",
       "28346             1.000000                   0.000000              -2.044563   \n",
       "24850             0.696036                   0.000000              -0.761931   \n",
       "2301              0.000000                   0.000000              -2.380800   \n",
       "42220             0.307739                   0.692261               0.258746   \n",
       "15621             1.000000                   1.000000               0.258746   \n",
       "\n",
       "       norm_feature_65  norm_feature_29  norm_feature_64  norm_feature_44  \\\n",
       "28346        -0.005440         0.108638         0.014463         0.135921   \n",
       "24850        -0.248382        -0.652147        -0.385947        -0.692244   \n",
       "2301         -0.746978         1.146315         0.674171         1.133739   \n",
       "42220        -0.162771        -0.675146         0.326701        -0.662817   \n",
       "15621        -0.275564        -0.675038         1.935202        -0.639324   \n",
       "\n",
       "          ...       norm_feature_3  norm_mean_last_pmt_acc_opnd  \\\n",
       "28346     ...             1.212834                     2.539525   \n",
       "24850     ...             1.538943                    -0.029341   \n",
       "2301      ...             0.082505                    -0.202718   \n",
       "42220     ...            -0.229240                    -0.605799   \n",
       "15621     ...            -0.859407                    -0.692453   \n",
       "\n",
       "       norm_enquiry_recency_365  norm_enquiry_recency_90  norm_feature_34  \\\n",
       "28346                 -0.598901                -0.598901        -0.555239   \n",
       "24850                  0.922180                 0.922180         1.063801   \n",
       "2301                   5.226114                 5.226114         1.770848   \n",
       "42220                 -0.351662                -0.351662        -0.555239   \n",
       "15621                 -0.739956                -0.739956        -0.555239   \n",
       "\n",
       "       feature_58_N  norm_feature_7  norm_min_months_before_first_30dpd  \\\n",
       "28346           1.0       -0.530242                            2.184268   \n",
       "24850           1.0       -0.106040                           -0.363198   \n",
       "2301            1.0       -1.149356                           -0.577087   \n",
       "42220           1.0       -0.055496                           -0.522950   \n",
       "15621           1.0       -0.775151                            0.478410   \n",
       "\n",
       "       Bad_label  customer_no  \n",
       "28346          1        28347  \n",
       "24850          1        24851  \n",
       "2301           0         2302  \n",
       "42220          1        42221  \n",
       "15621          0        15622  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_select += ['Bad_label', 'customer_no']\n",
    "train_df_blncd_new = train_df_blncd[cols_to_select]\n",
    "train_df_blncd_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=150, random_state=None)"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the parameters\n",
    "n_estimators = 150\n",
    "learning_rate = 1.\n",
    "\n",
    "# train the adaboost algorithm\n",
    "ada_real_2 = AdaBoostClassifier(\n",
    "    learning_rate=learning_rate,\n",
    "    n_estimators=n_estimators,\n",
    "    algorithm=\"SAMME.R\")\n",
    "\n",
    "ada_real_2.fit(train_df_blncd_new.iloc[:,:-2], train_df_blncd_new.iloc[:,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df_new = test_df[cols_to_select]\n",
    "\n",
    "# Predictions\n",
    "predictions_ab_2 = ada_real_2.predict(test_df_new.iloc[:,:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Good Customer       0.96      0.95      0.95      9778\n",
      " Bad Customer       0.08      0.10      0.09       462\n",
      "\n",
      "  avg / total       0.92      0.91      0.91     10240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get classification report\n",
    "target_names=['Good Customer', 'Bad Customer']\n",
    "print(classification_report(test_df_new.loc[:,'Bad_label'], predictions_ab_2, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score: 0.5224773521971313, Gini: 0.044954704394262635\n"
     ]
    }
   ],
   "source": [
    "get_auc_gini(actual=test_df_new.loc[:,'Bad_label'].tolist(), prediction=list(predictions_ab_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mmmmmm sad that it didn't go with my intuition:( :(\n",
    "\n",
    "Anyway that was a real fun I had doing this project.\n",
    "\n",
    "A Gini of close to 20 is not bad though with the time that I've taken to finish this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you!! Had a great recollecting time doing this project:) :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
